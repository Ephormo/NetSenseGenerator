from flask import Flask, render_template, request, jsonify,url_for
import jieba
import pandas as pd
import pinyin
from openpyxl import load_workbook
import re
import os
import ollama
from pathlib import Path
from typing import List, Dict
import logging
from fuzzywuzzy import fuzz
import asyncio


app = Flask(__name__)

# åˆå§‹åŒ–jiebaåˆ†è¯
jieba.initialize()

def load_dictionary():
    try:
        wb = load_workbook('data/bible_new.xlsx')
        ws = wb.active
        raw_col = None
        cx_col = None

        for col in ws.iter_cols(max_row=1):
            if 'raw' in str(col[0].value).lower():
                raw_col = col[0].column_letter
            elif 'chouxiang' in str(col[0].value).lower():
                cx_col = col[0].column_letter

        if not raw_col or not cx_col:
            raise ValueError("æœªæ‰¾åˆ°'raw'æˆ–'chouxiang'åˆ—")

        bible_light_dict = {}
        bible_deep_dict = {}

        for row in ws.iter_rows(min_row=2):
            raw = str(row[ord(raw_col) - 65].value).strip()
            cx = str(row[ord(cx_col) - 65].value).strip()

            if raw and cx:
                bible_light_dict[raw] = cx
                try:
                    py = pinyin.get(raw, format='strip') if not raw.isdigit() else raw
                    bible_deep_dict[py] = cx
                except:
                    bible_deep_dict[raw] = cx

        print("\nå­—å…¸åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ£€æŸ¥ï¼š")
        for i, (k, v) in enumerate(bible_light_dict.items()):
            if i >= 3: break
            print(f"'{k}' â†’ '{v}'")

        return bible_light_dict, bible_deep_dict

    except Exception as e:
        print(f"\nâš ï¸ åŠ è½½å­—å…¸å‡ºé”™: {str(e)}")
        return {"ä½ å¥½": "ğŸ˜Š", "å¼€å¿ƒ": "ğŸ˜„"}, {"nihao": "ğŸ˜Š", "kaixin": "ğŸ˜„"}

bible_light_dict, bible_deep_dict = load_dictionary()

def text_to_emoji(text):
    result = []
    for word in jieba.cut(text):
        word = word.strip()

        if word in bible_light_dict:
            result.append(bible_light_dict[word])
            continue

        try:
            word_py = pinyin.get(word, format='strip')
            if word_py in bible_deep_dict:
                result.append(bible_deep_dict[word_py])
                continue
        except:
            pass

        for char in word:
            if char in bible_light_dict:
                result.append(bible_light_dict[char])
            else:
                try:
                    char_py = pinyin.get(char, format='strip')
                    result.append(bible_deep_dict.get(char_py, char))
                except:
                    result.append(char)

    return ''.join(result)



# Emoji æ˜ å°„åº“ï¼ˆå¯è‡ªå®šä¹‰æ‰©å±•ï¼‰
EMOJI_MAPPING = {
    'å¼€å¿ƒ': 'ğŸ˜Š', 'é«˜å…´': 'ğŸ˜„', 'å¿«ä¹': 'ğŸ˜€', 'ç¬‘': 'ğŸ˜‚',
    'ç”Ÿæ°”': 'ğŸ˜ ', 'æ„¤æ€’': 'ğŸ¤¬', 'è®¨åŒ': 'ğŸ˜¤',
    'æ‚²ä¼¤': 'ğŸ˜¢', 'éš¾è¿‡': 'ğŸ˜­', 'å“­': 'ğŸ˜­',
    'æƒŠè®¶': 'ğŸ˜²', 'éœ‡æƒŠ': 'ğŸ¤¯', 'æ„å¤–': 'ğŸ˜®',
    'çˆ±': 'â¤ï¸', 'å–œæ¬¢': 'ğŸ¥°', 'å¿ƒ': 'ğŸ’–',
    'ç–‘é—®': 'â“', 'é—®é¢˜': 'ğŸ¤”', 'ä¸ºä»€ä¹ˆ': 'â‰ï¸',
    'æ—¶é—´': 'â°', 'æ—©ä¸Š': 'ğŸŒ…', 'æ™šä¸Š': 'ğŸŒƒ',
    'å¤©æ°”': 'â˜€ï¸', 'é›¨': 'ğŸŒ§ï¸', 'é›ª': 'â„ï¸',
    'åƒ': 'ğŸ”', 'å–': 'ğŸ¹', 'ç¾é£Ÿ': 'ğŸ•',
    'åŠ¨ç‰©': 'ğŸ¶', 'çŒ«': 'ğŸ±', 'ç‹—': 'ğŸ•',
    'å·¥ä½œ': 'ğŸ’¼', 'å­¦ä¹ ': 'ğŸ“š', 'é’±': 'ğŸ’°',
    'é»˜è®¤': 'âœ¨', 'å®¶': 'ğŸ ', 'è½¦': 'ğŸš—', 'é£æœº': 'âœˆï¸',
    'ç«è½¦': 'ğŸš†', 'èˆ¹': 'ğŸš¢', 'è‡ªè¡Œè½¦': 'ğŸš²', 'å…¬äº¤è½¦': 'ğŸšŒ',
    'åœ°é“': 'ğŸš‡', 'å‡ºç§Ÿè½¦': 'ğŸš•', 'æ‘©æ‰˜è½¦': 'ğŸï¸', 'ç«ç®­': 'ğŸš€',
    'ç”µè¯': 'ğŸ“', 'æ‰‹æœº': 'ğŸ“±', 'ç”µè„‘': 'ğŸ’»', 'ç”µè§†': 'ğŸ“º',
    'ç›¸æœº': 'ğŸ“·', 'éŸ³ä¹': 'ğŸµ', 'ç”µå½±': 'ğŸ¬', 'ä¹¦': 'ğŸ“–',
    'ç¤¼ç‰©': 'ğŸ', 'ç”Ÿæ—¥': 'ğŸ‚', 'åœ£è¯': 'ğŸ„', 'æ–°å¹´': 'ğŸ‰',
    'æ´¾å¯¹': 'ğŸŠ', 'è¿åŠ¨': 'âš½', 'ç¯®çƒ': 'ğŸ€', 'è¶³çƒ': 'âš½',
    'ç½‘çƒ': 'ğŸ¾', 'æ£’çƒ': 'âš¾', 'é«˜å°”å¤«': 'â›³', 'æ¸¸æ³³': 'ğŸŠ',
    'è·‘æ­¥': 'ğŸƒ', 'å¥èº«': 'ğŸ‹ï¸', 'ç‘œä¼½': 'ğŸ§˜', 'æ‹³å‡»': 'ğŸ¥Š',
    'æ»‘é›ª': 'â›·ï¸', 'æ»‘å†°': 'â›¸ï¸', 'å†²æµª': 'ğŸ„', 'éª‘é©¬': 'ğŸ‡',
    'é’“é±¼': 'ğŸ£', 'ç™»å±±': 'ğŸ§—', 'éœ²è¥': 'ğŸ•ï¸', 'æ—…è¡Œ': 'ğŸ§³',
    'åœ°å›¾': 'ğŸ—ºï¸', 'æŒ‡å—é’ˆ': 'ğŸ§­', 'é…’åº—': 'ğŸ¨', 'é¤å…': 'ğŸ½ï¸',
    'å’–å•¡': 'â˜•', 'èŒ¶': 'ğŸµ', 'å•¤é…’': 'ğŸº', 'è‘¡è„é…’': 'ğŸ·',
    'é¸¡å°¾é…’': 'ğŸ¸', 'å†°æ·‡æ·‹': 'ğŸ¦', 'è›‹ç³•': 'ğŸ°', 'å·§å…‹åŠ›': 'ğŸ«',
    'ç³–æœ': 'ğŸ¬', 'é¥¼å¹²': 'ğŸª', 'é¢åŒ…': 'ğŸ', 'æŠ«è¨': 'ğŸ•',
    'æ±‰å ¡': 'ğŸ”', 'çƒ­ç‹—': 'ğŸŒ­', 'è–¯æ¡': 'ğŸŸ', 'å¯¿å¸': 'ğŸ£',
    'æ‹‰é¢': 'ğŸœ', 'æ²™æ‹‰': 'ğŸ¥—', 'æ°´æœ': 'ğŸ', 'è”¬èœ': 'ğŸ¥¦',
    'è‚‰': 'ğŸ–', 'é±¼': 'ğŸŸ', 'è™¾': 'ğŸ¦', 'èƒèŸ¹': 'ğŸ¦€',
    'é¾™è™¾': 'ğŸ¦', 'ç« é±¼': 'ğŸ™', 'è´å£³': 'ğŸš', 'èŠ±': 'ğŸŒ¸',
    'æ ‘': 'ğŸŒ³', 'è‰': 'ğŸŒ¿', 'å¶å­': 'ğŸƒ', 'å¤ªé˜³': 'ğŸŒ',
    'æœˆäº®': 'ğŸŒœ', 'æ˜Ÿæ˜Ÿ': 'â­', 'äº‘': 'â˜ï¸', 'é›¨ä¼': 'â˜‚ï¸',
    'å½©è™¹': 'ğŸŒˆ', 'ç«': 'ğŸ”¥', 'æ°´': 'ğŸ’§', 'å†°': 'ğŸ§Š',
    'é›ªäºº': 'â›„', 'é£': 'ğŸŒ¬ï¸', 'é—ªç”µ': 'âš¡', 'å±±': 'â›°ï¸',
    'æµ·': 'ğŸŒŠ', 'æ²™æ¼ ': 'ğŸœï¸', 'æ£®æ—': 'ğŸŒ²', 'å²›': 'ğŸï¸',
    'åŸå¸‚': 'ğŸ™ï¸', 'å»ºç­‘': 'ğŸ¢', 'æ¡¥': 'ğŸŒ‰', 'å¡”': 'ğŸ—¼',
    'é›•åƒ': 'ğŸ—½', 'æ•™å ‚': 'â›ª', 'å¯ºåº™': 'ğŸ›•', 'æ¸…çœŸå¯º': 'ğŸ•Œ',
    'åŸå ¡': 'ğŸ°', 'å­¦æ ¡': 'ğŸ«', 'åŒ»é™¢': 'ğŸ¥', 'é“¶è¡Œ': 'ğŸ¦',
    'é‚®å±€': 'ğŸ¤', 'è­¦å¯Ÿå±€': 'ğŸš“', 'æ¶ˆé˜²å±€': 'ğŸš’', 'å›¾ä¹¦é¦†': 'ğŸ“š',
    'åšç‰©é¦†': 'ğŸ›ï¸', 'å‰§é™¢': 'ğŸ­', 'ç”µå½±é™¢': 'ğŸ¦', 'å•†åº—': 'ğŸ¬',
    'è¶…å¸‚': 'ğŸ›’', 'å¸‚åœº': 'ğŸ›ï¸', 'å…¬å›­': 'ğŸï¸', 'åŠ¨ç‰©å›­': 'ğŸ¦',
    'æ¸¸ä¹å›­': 'ğŸ¡', 'æ°´æ—é¦†': 'ğŸ ', 'æ¤ç‰©å›­': 'ğŸŒº', 'å†œåœº': 'ğŸšœ',
    'å·¥å‚': 'ğŸ­', 'æœºåœº': 'ğŸ›«', 'ç«è½¦ç«™': 'ğŸš‰', 'åœ°é“ç«™': 'ğŸš‡',
    'å…¬äº¤ç«™': 'ğŸš', 'åŠ æ²¹ç«™': 'â›½', 'åœè½¦åœº': 'ğŸ…¿ï¸', 'å•æ‰€': 'ğŸš½',
    'æµ´å®¤': 'ğŸ›', 'å¨æˆ¿': 'ğŸ³', 'å§å®¤': 'ğŸ›ï¸', 'å®¢å…': 'ğŸ›‹ï¸',
    'åŠå…¬å®¤': 'ğŸ¢', 'ä¼šè®®å®¤': 'ğŸ¢', 'å®éªŒå®¤': 'ğŸ”¬', 'æ•™å®¤': 'ğŸ«',
    'æ“åœº': 'ğŸŸï¸', 'ä½“è‚²é¦†': 'ğŸŸï¸', 'æ¸¸æ³³æ± ': 'ğŸŠ', 'å¥èº«æˆ¿': 'ğŸ‹ï¸',
    'è¯Šæ‰€': 'ğŸ¥', 'è¯åº—': 'ğŸ’Š', 'ç¾å®¹é™¢': 'ğŸ’…', 'ç†å‘åº—': 'ğŸ’‡',
    'æ´—è¡£åº—': 'ğŸ§º', 'ä¿®ç†åº—': 'ğŸ”§', 'è½¦åº“': 'ğŸš—', 'ä»“åº“': 'ğŸ“¦',
    'å·¥åœ°': 'ğŸ—ï¸', 'ç å¤´': 'ğŸš¢', 'æ¸¯å£': 'âš“', 'ç¯å¡”': 'ğŸš¨',
    'ä¿¡å·ç¯': 'ğŸš¦', 'è·¯æ ‡': 'ğŸš§', 'äº¤é€š': 'ğŸš¥', 'å®‰å…¨': 'ğŸ›¡ï¸',
    'è­¦å‘Š': 'âš ï¸', 'ç¦æ­¢': 'ğŸš«', 'å…è®¸': 'âœ…', 'å¸®åŠ©': 'ğŸ†˜',
    'æ€¥æ•‘': 'ğŸš‘', 'å¥åº·': 'ğŸ’‰', 'è¯å“': 'ğŸ’Š', 'åŒ»ç”Ÿ': 'ğŸ‘¨â€âš•ï¸',
    'æŠ¤å£«': 'ğŸ‘©â€âš•ï¸', 'ç—…äºº': 'ğŸ¤’', 'å—ä¼¤': 'ğŸ¤•', 'åº·å¤': 'ğŸ’ª',
    'è¿åŠ¨å‘˜': 'ğŸƒ', 'æ•™ç»ƒ': 'ğŸ‹ï¸', 'è£åˆ¤': 'âš–ï¸', 'å† å†›': 'ğŸ†',
    'å¥–ç‰Œ': 'ğŸ¥‡', 'å¥–æ¯': 'ğŸ†', 'æ¯”èµ›': 'ğŸ…', 'è®­ç»ƒ': 'ğŸ‹ï¸',
    'å›¢é˜Ÿ': 'ğŸ‘¥', 'åˆä½œ': 'ğŸ¤', 'ç«äº‰': 'âš”ï¸', 'èƒœåˆ©': 'ğŸ‰',
    'å¤±è´¥': 'ğŸ˜', 'åŠªåŠ›': 'ğŸ’ª', 'åšæŒ': 'âœŠ', 'æ¢¦æƒ³': 'ğŸŒŸ',
    'å¸Œæœ›': 'ğŸŒˆ', 'ä¿¡å¿µ': 'ğŸ™', 'å‹‡æ°”': 'ğŸ¦', 'æ™ºæ…§': 'ğŸ§ ',
    'çŸ¥è¯†': 'ğŸ“š', 'å­¦ä¹ ': 'ğŸ“–', 'æ•™è‚²': 'ğŸ«', 'è€ƒè¯•': 'ğŸ“',
    'æˆç»©': 'ğŸ“Š', 'è¯ä¹¦': 'ğŸ“', 'æ¯•ä¸š': 'ğŸ“', 'å·¥ä½œ': 'ğŸ’¼',
    'èŒä¸š': 'ğŸ‘”', 'äº‹ä¸š': 'ğŸ“ˆ', 'æˆåŠŸ': 'ğŸ†', 'å¤±è´¥': 'ğŸ˜',
    'æŒ‘æˆ˜': 'ğŸ’ª', 'æœºä¼š': 'ğŸŒŸ', 'é£é™©': 'âš ï¸', 'å®‰å…¨': 'ğŸ›¡ï¸',
    'å¥åº·': 'ğŸ’‰',
    # æ²¡æœ‰åŒ¹é…åˆ°å…³é”®è¯æ—¶ä½¿ç”¨çš„é»˜è®¤emoji
}

def analyze_sentence(sentence):
    sentence_lower = sentence.lower()
    for keyword, emoji in EMOJI_MAPPING.items():
        if keyword in sentence_lower:
            return emoji
    return EMOJI_MAPPING.get('é»˜è®¤', '')

def add_emoji_to_text(text):
    punctuations = r'([ã€‚ï¼ï¼Ÿï¼Œï¼›])'
    parts = re.split(punctuations, text)
    result = []

    for i, part in enumerate(parts):
        if part in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï¼›']:
            if i > 0 and parts[i - 1].strip():
                sentence = parts[i - 1]
                emoji = analyze_sentence(sentence)
                result[-1] = result[-1] + emoji
            result.append(part)
        else:
            result.append(part)

    return ''.join(result)




# function3

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedMemeAssistant:
    def __init__(self, excel_path: str, model: str = "qwen:7b"):
        excel_path='data/memes_dataset.xlsx'
        model='qwen'
        self.excel_path = Path(excel_path)
        self.model = model
        self.meme_db = self._load_meme_database()
        self.all_keywords = self._extract_all_keywords()

    def _load_meme_database(self) -> List[Dict]:
        """åŠ è½½Excelä¸­çš„æ¢—æ•°æ®åº“"""
        try:
            df = pd.read_excel(self.excel_path)
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            if not all(col in df.columns for col in ["æ¢—åç§°", "æ‘˜è¦", "æ ‡ç­¾"]):
                raise ValueError("Excelå¿…é¡»åŒ…å«'æ¢—åç§°','æ‘˜è¦','æ ‡ç­¾'ä¸‰åˆ—")

            # é¢„å¤„ç†æ•°æ®
            records = df.to_dict("records")
            for item in records:
                # å¤„ç†æ ‡ç­¾åˆ—ï¼Œç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
                if isinstance(item["æ ‡ç­¾"], str):
                    # å¤„ç†å¤šç§åˆ†éš”ç¬¦æƒ…å†µ
                    tags = re.sub(r"[\[\]'\"]", "", item["æ ‡ç­¾"])
                    item["æ ‡ç­¾"] = [tag.strip() for tag in re.split(r"[,ï¼Œ\s]+", tags) if tag.strip()]
                elif pd.isna(item["æ ‡ç­¾"]):
                    item["æ ‡ç­¾"] = []
            return records
        except Exception as e:
            logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
            raise RuntimeError(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")

    def _extract_all_keywords(self) -> List[str]:
        """æå–æ‰€æœ‰å…³é”®è¯(æ¢—åç§°+æ ‡ç­¾)ç”¨äºå¿«é€ŸåŒ¹é…"""
        keywords = set()
        for item in self.meme_db:
            keywords.add(item["æ¢—åç§°"])
            for tag in item["æ ‡ç­¾"]:
                keywords.add(tag.lower())
        return list(keywords)

    def _find_best_match(self, query: str) -> Dict:
        """æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€åŒ¹é…çš„æ¢—(ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…)"""
        best_match = None
        highest_score = 0

        for item in self.meme_db:
            # è®¡ç®—æ¢—åç§°åŒ¹é…åº¦
            name_score = fuzz.token_set_ratio(query.lower(), item["æ¢—åç§°"])

            # è®¡ç®—æ ‡ç­¾åŒ¹é…åº¦(å–æœ€é«˜åˆ†æ ‡ç­¾)
            tag_score = max(
                [fuzz.token_set_ratio(query.lower(), tag.lower()) for tag in item["æ ‡ç­¾"]] or [0]
            )

            # ç»¼åˆè¯„åˆ†(åç§°æƒé‡æ›´é«˜)
            total_score = name_score * 0.7 + tag_score * 0.3

            if total_score > highest_score:
                highest_score = total_score
                best_match = item

        return best_match, highest_score

    def _find_related_matches(self, query: str, exclude: str, threshold: int = 40) -> List[Dict]:
        """æ‰¾åˆ°ç›¸å…³çš„æ¢—(æ’é™¤æœ€ä½³åŒ¹é…)"""
        related = []

        for item in self.meme_db:
            if item["æ¢—åç§°"] == exclude:
                continue

            # è®¡ç®—æ¢—åç§°åŒ¹é…åº¦
            name_score = fuzz.token_set_ratio(query.lower(), item["æ¢—åç§°"])

            # è®¡ç®—æ ‡ç­¾åŒ¹é…åº¦(å–æœ€é«˜åˆ†æ ‡ç­¾)
            tag_score = max(
                [fuzz.token_set_ratio(query.lower(), tag.lower()) for tag in item["æ ‡ç­¾"]] or [0]
            )

            # ç»¼åˆè¯„åˆ†
            total_score = name_score * 0.5 + tag_score * 0.5

            if total_score >= threshold:
                related.append((item, total_score))

        # æŒ‰åŒ¹é…åº¦æ’åºå¹¶è¿”å›å‰3ä¸ª
        related.sort(key=lambda x: x[1], reverse=True)
        return [item for item, score in related[:3]]

    async def ask(self, query: str) -> str:
        """
        å‘åŠ©æ‰‹æé—®

        å‚æ•°:
            query: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜(ä¸­æ–‡)

        è¿”å›:
            åŠ©æ‰‹çš„å›ç­”(åŒ…å«æœ€ä½³åŒ¹é…å’Œç›¸å…³æ¨è)
        """
        # 1. æ‰¾åˆ°æœ€ä½³åŒ¹é…
        best_match, match_score = self._find_best_match(query)

        response = ""

        # å¦‚æœæœ‰è¾ƒå¥½çš„åŒ¹é…(åˆ†æ•°>50)
        if best_match and match_score > 50:
            response += f"ğŸ¯ æœ€åŒ¹é…çš„ç»“æœ(åŒ¹é…åº¦{match_score}%):\n"
            response += f"ã€{best_match['æ¢—åç§°']}ã€‘\n{best_match['æ‘˜è¦']}\n"
            if best_match["æ ‡ç­¾"]:
                response += f"ğŸ·ï¸ ç›¸å…³æ ‡ç­¾: {'ã€'.join(best_match['æ ‡ç­¾'])}\n"
            response += "\n"

            # 2. æŸ¥æ‰¾ç›¸å…³æ¨è
            related_memes = self._find_related_matches(query, best_match["æ¢—åç§°"])

            if related_memes:
                response += "ğŸ” æ‚¨å¯èƒ½è¿˜å¯¹ä»¥ä¸‹æ¢—æ„Ÿå…´è¶£:\n\n"
                for meme in related_memes:
                    response += f"â–ª {meme['æ¢—åç§°']}: {meme['æ‘˜è¦'][:60]}...\n"
                    if meme["æ ‡ç­¾"]:
                        response += f"  æ ‡ç­¾: {'ã€'.join(meme['æ ‡ç­¾'][:3])}\n"
                    response += "\n"
        else:
            # 3. æ²¡æœ‰è¶³å¤Ÿå¥½çš„åŒ¹é…æ—¶è°ƒç”¨Ollama
            try:
                prompt = (
                    f"ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡ç½‘ç»œçƒ­æ¢—çŸ¥è¯†åŠ©æ‰‹ã€‚ç”¨æˆ·é—®: {query}\n"
                    "è¯·ç”¨ç®€æ´æ˜äº†çš„ä¸­æ–‡å›ç­”å…³äºç½‘ç»œæµè¡Œæ¢—çš„é—®é¢˜ã€‚"
                    "å¦‚æœä¸çŸ¥é“ç¡®åˆ‡ç­”æ¡ˆï¼Œå¯ä»¥ç»™å‡ºåˆç†çš„æ¨æµ‹æˆ–ç›¸å…³æ¢—çš„ä»‹ç»ã€‚"
                )

                ai_response = await ollama.chat(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}]
                )
                response = ai_response['message']['content']

                # å³ä½¿è°ƒç”¨AIä¹Ÿå°è¯•æä¾›ä¸€äº›å¯èƒ½ç›¸å…³çš„æ¢—
                related_memes = self._find_related_matches(query, "", threshold=30)
                if related_memes:
                    response += "\n\nğŸ” ä»¥ä¸‹å¯èƒ½ç›¸å…³çš„ç½‘ç»œæ¢—:\n"
                    for meme in related_memes:
                        response += f"â–ª {meme['æ¢—åç§°']}: {meme['æ‘˜è¦'][:50]}...\n"
            except Exception as e:
                logger.error(f"è°ƒç”¨Ollama APIå¤±è´¥: {e}")
                response = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æ‚¨å¯ä»¥å°è¯•æ¢ç§æ–¹å¼æé—®ã€‚"

                # ä»ç„¶å°è¯•æä¾›ä¸€äº›å¯èƒ½ç›¸å…³çš„æ¢—
                related_memes = self._find_related_matches(query, "", threshold=20)
                if related_memes:
                    response += "\n\nä»¥ä¸‹å¯èƒ½ç›¸å…³çš„ç½‘ç»œæ¢—:\n"
                    for meme in related_memes:
                        response += f"â–ª {meme['æ¢—åç§°']}\n"

        return response






# flask

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/page1')
def page1():
    return render_template('page1.html')

@app.route('/page2')
def page2():
    return render_template('page2.html')

@app.route('/page3')
def page3():
    return render_template('page3.html')

@app.route('/convert', methods=['POST'])
def convert():
    try:
        data = request.get_json()
        text = data.get('text', '')
        if not text:
            return jsonify({'error': 'è¯·è¾“å…¥æ–‡æœ¬'}), 400

        result = text_to_emoji(text)
        print(f"è½¬æ¢ç»“æœ: {text} â†’ {result}")
        return jsonify({'result': result})

    except Exception as e:
        print(f"è½¬æ¢å‡ºé”™: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/add_emoji', methods=['POST'])
def add_emoji():
    try:
        data = request.get_json()
        text = data.get('text', '')
        if not text:
            return jsonify({'error': 'è¯·è¾“å…¥æ–‡æœ¬'}), 400

        result = add_emoji_to_text(text)
        print(f"æ·»åŠ Emojiç»“æœ: {text} â†’ {result}")
        return jsonify({'result': result})

    except Exception as e:
        print(f"æ·»åŠ Emojiå‡ºé”™: {str(e)}")
        return jsonify({'error': str(e)}), 500


# function3
meme_searcher = EnhancedMemeAssistant("memes_dataset.xlsx")

@app.route('/lookup', methods=['POST'])
def lookup():
    try:
        data = request.get_json()
        query = data.get('text', '')
        if not query:
            return jsonify({'error': 'è¯·è¾“å…¥æ–‡æœ¬'}), 400

        # ä½¿ç”¨ asyncio è°ƒç”¨ async æ–¹æ³•
        result = asyncio.run(meme_searcher.ask(query))
        return jsonify({'result': result})

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å‡ºé”™: {str(e)}")
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    os.makedirs('data', exist_ok=True)
    app.run(debug=True, port=5001)





